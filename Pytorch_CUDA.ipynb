{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMTvDHG9AZGakh1YMi0u5hr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OluwoleDove/medical_image_classifier/blob/main/Pytorch_CUDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available. PyTorch is using the GPU!\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"CUDA is not available. PyTorch will use the CPU.\")\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e3QORWQzEQ9",
        "outputId": "60cecae8-7fcf-4b48-b16f-274e1a0efe66"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. PyTorch is using the GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y8sfePcKy2gw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load the MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "# Load data into DataLoader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU49L2m_zftv",
        "outputId": "6996d66c-33db-4524-de0d-43e22c217649"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 16361021.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 496818.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4509344.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4854874.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "62fApsNlzpjC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the network and move it to GPU if available\n",
        "model = Net().to(device)"
      ],
      "metadata": {
        "id": "-F7oNMiZzu-o"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "-B9ceB9QzyvY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)  # Move data to GPU\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        output = model(data)  # Forward pass\n",
        "        loss = criterion(output, target)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}'\n",
        "                  f' ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')"
      ],
      "metadata": {
        "id": "hq_AE5L3z2re"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)  # Move data to GPU\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # Sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "          f' ({100. * correct / len(test_loader.dataset):.0f}%)\\n')"
      ],
      "metadata": {
        "id": "F7qFpxYnz7il"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d83vWy5gz_5b",
        "outputId": "11a5e5ae-37a7-4ac0-8d63-8ba1380d97cf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.308317\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.145975\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.106815\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.198163\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.155490\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.050165\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.162102\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.036709\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.032853\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.012841\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9842/10000 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.105967\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.019798\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.109971\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.070471\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.043568\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.002676\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.144793\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.094404\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.076143\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.033103\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9851/10000 (99%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.091636\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.033384\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.026904\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.006855\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.318276\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.016407\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.016389\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.036252\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.002025\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.089156\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9893/10000 (99%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.011865\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.004139\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.007514\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.033841\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.003373\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.013286\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.000260\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.009575\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.083656\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.001250\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9902/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.000712\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.003139\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.001965\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.004844\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.000426\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.003360\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.018000\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.035707\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.000653\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.003389\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9892/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"mnist_cnn.pth\")"
      ],
      "metadata": {
        "id": "kWFxkALm0Weg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TO LOAD THE MODEL LATER\n",
        "model = Net().to(device)\n",
        "model.load_state_dict(torch.load(\"mnist_cnn.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ngn9QpB0cHc",
        "outputId": "cc2c90d0-8167-4eec-d195-3de01556ccd0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-668c86614cb1>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"mnist_cnn.pth\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}